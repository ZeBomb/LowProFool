{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26efdaf83be2471aab71a20b848a9e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71bfb2d665a242a68515a98f46867ccc",
              "IPY_MODEL_3b86a3ca198548c8bd7edd3ee630bfc9",
              "IPY_MODEL_f4f301a017664476a87afa85a4032913"
            ],
            "layout": "IPY_MODEL_0b18593212ca4633ba02c877c71d734b"
          }
        },
        "71bfb2d665a242a68515a98f46867ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34da8fd33d73436c873de3e79f23c640",
            "placeholder": "​",
            "style": "IPY_MODEL_bc6fbe2d6e3a442e8b880c2d924aeb44",
            "value": "LowProFool: 100%"
          }
        },
        "3b86a3ca198548c8bd7edd3ee630bfc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de5eccc509df4f268726bce77b500627",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0709cf41db145279cb92b016be706fd",
            "value": 150
          }
        },
        "f4f301a017664476a87afa85a4032913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37ecaba209643ab868b9bcb4deca9c2",
            "placeholder": "​",
            "style": "IPY_MODEL_fa789d9ae1904afaa11c5617620c6163",
            "value": " 150/150 [1:11:26&lt;00:00, 28.11s/it]"
          }
        },
        "0b18593212ca4633ba02c877c71d734b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34da8fd33d73436c873de3e79f23c640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc6fbe2d6e3a442e8b880c2d924aeb44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de5eccc509df4f268726bce77b500627": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0709cf41db145279cb92b016be706fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d37ecaba209643ab868b9bcb4deca9c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa789d9ae1904afaa11c5617620c6163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av30yzN5t2sr"
      },
      "outputs": [],
      "source": [
        "# Misc\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "from tqdm import tqdm_notebook\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "from torch import tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Dy6gPNe7uTKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sklearn\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "EUslgWqsuVFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "# Keras \n",
        "import keras"
      ],
      "metadata": {
        "id": "yWIOnbGvuXW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpers\n",
        "# from Adverse import lowProFool, deepfool\n",
        "# from Metrics import *\n",
        "# Misc\n",
        "import numpy as np\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "# from torch.autograd.gradcheck import zero_gradients\n",
        "def zero_gradients(x):\n",
        "  if x.grad is not None:\n",
        "      x.grad.zero_()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import sklearn\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "def get_metrics(config, list_metrics, n_neighbors=2):\n",
        "    \"\"\"\n",
        "    Generates an adversarial examples x' from an original sample x. Expected to contain\n",
        "            Dataset, MaxIters, Alpha, Lambda, TrainData, TestData, ValidData, Scaler,\n",
        "            FeatureNames, Target, Weights, Bounds, Model\n",
        "    :param config: dictionnary containing the configuration for the experiment\n",
        "    :param list_metrics: dictionnary containing the metrics to be computed. Choose from\n",
        "            from SuccessRate, iter_means, iter_std, normdelta_median, normdelta_mean,\n",
        "            n_std, weighted_median, weighted_mean, w_std, mean_dists_at_org,\n",
        "            median_dists_at_org, mean_dists_at_tgt, mean_dists_at_org_weighted, mdow_std,\n",
        "            median_dists_at_org_weighted, mean_dists_at_tgt_weighted, mdtw_std, prop_same_class_arg_org,\n",
        "            prop_same_class_arg_adv,\n",
        "    :param n_neighbors: number of neighbors to compute the distance to n_neighbors closest neighbors\n",
        "    \"\"\"\n",
        "\n",
        "    metrics_for_conf = []\n",
        "    df_test = config['TestData']\n",
        "    dfs_adv = config['AdvData']\n",
        "    \n",
        "    for method, df_adv in dfs_adv.items():\n",
        "        metrics_for_method = [method]\n",
        "        # Get success rate before removing samples from dataframe\n",
        "        if list_metrics['SuccessRate']:\n",
        "            sr = metric_success_rate_for(df_adv)\n",
        "            metrics_for_method.append(sr)\n",
        "\n",
        "        # Removes samples that did cross frontier\n",
        "        df_adv = remove_non_converted(df_adv)        \n",
        "        df_adv = add_normdelta_to(df_adv, config, df_test)\n",
        "\n",
        "        # Adding proportion of neighbors from diff classes\n",
        "        df_adv, df_adv_weighted = add_maj_neighbors(df_adv, df_test, config, n_neighbors=n_neighbors)            \n",
        "\n",
        "        # Mean, std, number of iterations\n",
        "        if list_metrics['iter_means']:\n",
        "            means_iters, stds_iters = mean_norm_for_col(df_adv, col='iters')\n",
        "            metrics_for_method.append(means_iters)\n",
        "            if list_metrics['iter_std']:\n",
        "                metrics_for_method.append(stds_iters)\n",
        "\n",
        "        # Median, norm of perturbation\n",
        "        if list_metrics['normdelta_median']:\n",
        "            median = median_norm_for_col(df_adv, col='normdelta')\n",
        "            metrics_for_method.append(median)\n",
        "\n",
        "        # Mean, std, norm of perturbation\n",
        "        if list_metrics['normdelta_mean']:\n",
        "            means, stds = mean_norm_for_col(df_adv, col='normdelta')\n",
        "            metrics_for_method.append(means)\n",
        "            if list_metrics['n_std']:\n",
        "                metrics_for_method.append(stds)\n",
        "\n",
        "        # Median, norm of perturbation, weighted\n",
        "        if list_metrics['weighted_median']:\n",
        "            median_w = median_norm_for_col(df_adv, col='normdelta_weighted')\n",
        "            metrics_for_method.append(median_w)\n",
        "\n",
        "        # Mean, std, norm of perturbation, weighted\n",
        "        if list_metrics['weighted_mean']:\n",
        "            means_w, stds_w = mean_norm_for_col(df_adv, col='normdelta_weighted')\n",
        "            metrics_for_method.append(means_w)\n",
        "            if list_metrics['w_std']:\n",
        "                metrics_for_method.append(stds_w) \n",
        "\n",
        "        # Mean, std, number of neighbors of a particular class at perturbed sample\n",
        "        if list_metrics['mean_dists_at_org']:\n",
        "            mean, std = mean_norm_for_col(df_adv, col='mean_dists_at_org')\n",
        "            metrics_for_method.append(mean)\n",
        "\n",
        "        # Mean, std, number of neighbors of a particular class at perturbed sample\n",
        "        if list_metrics['median_dists_at_org']:\n",
        "            med = median_norm_for_col(df_adv, col='mean_dists_at_org')\n",
        "            metrics_for_method.append(med)\n",
        "\n",
        "        # Mean, std, number of neighbors of a particular class at perturbed sample\n",
        "        if list_metrics['mean_dists_at_tgt']:\n",
        "            mean, std = mean_norm_for_col(df_adv, col='mean_dists_at_tgt')\n",
        "            metrics_for_method.append(mean)\n",
        "\n",
        "        # Mean, std, number of neighbors of a particular class at perturbed sample\n",
        "        if list_metrics['mean_dists_at_org_weighted']:\n",
        "            mean, std = mean_norm_for_col(df_adv_weighted, col='mean_dists_at_org')\n",
        "            metrics_for_method.append(mean)\n",
        "            if list_metrics['mdow_std']:\n",
        "                metrics_for_method.append(std)\n",
        "\n",
        "        # Mean, std, number of neighbors of a particular class at perturbed sample\n",
        "        if list_metrics['median_dists_at_org_weighted']:\n",
        "            median = median_norm_for_col(df_adv_weighted, col='mean_dists_at_org')\n",
        "            metrics_for_method.append(median)\n",
        "\n",
        "        # Mean, std, number of neighbors of a particular class at perturbed sample\n",
        "        if list_metrics['mean_dists_at_tgt_weighted']:\n",
        "            mean, std = mean_norm_for_col(df_adv_weighted, col='mean_dists_at_tgt')\n",
        "            metrics_for_method.append(mean)\n",
        "            if list_metrics['mdtw_std']:\n",
        "                metrics_for_method.append(std)\n",
        "\n",
        "        # Mean, std, number of neighbors of a particular class at perturbed sample\n",
        "        if list_metrics['prop_same_class_arg_org']:\n",
        "            mean, std = mean_norm_for_col(df_adv, col='prop_same_class_arg_org')\n",
        "            metrics_for_method.append(mean)\n",
        "\n",
        "        # Mean, std, number of neighbors of a particular class at perturbed sample\n",
        "        if list_metrics['prop_same_class_arg_adv']:\n",
        "            mean, std = mean_norm_for_col(df_adv, col='prop_same_class_arg_adv')\n",
        "            metrics_for_method.append(mean)\n",
        "            \n",
        "        metrics_for_conf.append(metrics_for_method)\n",
        "    return metrics_for_conf\n",
        "  \n",
        "def metric_success_rate_for(df):\n",
        "    return len(df[df['orig_pred'] != df['adv_pred']]) / df.shape[0]\n",
        "\n",
        "def remove_non_converted(df):\n",
        "    df_return = df.copy()\n",
        "    return df[df['orig_pred'] != df['adv_pred']]\n",
        "\n",
        "def mean_norm_for_col(df, col):\n",
        "    tmp = df[col]    \n",
        "    mean, std = np.mean(tmp), np.std(tmp)\n",
        "    return (mean, std)\n",
        "\n",
        "def median_norm_for_col(df, col):\n",
        "    tmp = df[col]    \n",
        "    median = np.median(tmp)\n",
        "    return median\n",
        "\n",
        "def add_normdelta_to(df_adv, conf, df):\n",
        "    # Drop columns if already there\n",
        "    df_return = df_adv.copy()\n",
        "    if 'normdelta' in df_return.columns:\n",
        "        df_return = df_return.drop(columns='normdelta')\n",
        "    if 'normdelta_weighted' in df_return.columns:\n",
        "        df_return = df_return.drop(columns='normdelta_weighted')\n",
        "        \n",
        "    feature_names = conf['FeatureNames']\n",
        "    weights = conf['Weights']\n",
        "\n",
        "    norms = []\n",
        "    norms_weighted = []\n",
        "    \n",
        "    # Iterate over all rows\n",
        "    for index, row in df_return.iterrows():\n",
        "        orig = df.loc[index][feature_names].values\n",
        "        adv = row[feature_names].values \n",
        "        \n",
        "        # Compute deltas\n",
        "        delta = np.abs(orig-adv)\n",
        "        assert(len(delta) == len(weights))\n",
        "        \n",
        "        # Norms delta\n",
        "        norms.append(np.linalg.norm(delta))\n",
        "        \n",
        "        # Norms delta weighted\n",
        "        norms_weighted.append(np.linalg.norm(delta * weights))\n",
        "\n",
        "    df_return.insert(0, 'normdelta', norms)\n",
        "    df_return.insert(0, 'normdelta_weighted', norms_weighted)\n",
        "    \n",
        "    return df_return\n",
        "\n",
        "\n",
        "def get_majority_neighbors(df_adv, df_orig, conf, knn, n_neighbors):\n",
        "    \n",
        "    # orig, adv\n",
        "    mean_dists = [[], []]\n",
        "    prop_same_class = [[], []]\n",
        "    \n",
        "    feature_names = conf['FeatureNames']\n",
        "    target = conf['Target']    \n",
        "    \n",
        "    # For each sample\n",
        "    for index, row in df_adv.iterrows():\n",
        "        \n",
        "        orig = df_orig.loc[index][feature_names].values\n",
        "        adv = row[feature_names].values\n",
        "        \n",
        "        preds = [row['orig_pred'], row['adv_pred']]\n",
        "        samples = [orig, adv]\n",
        "        \n",
        "        for i in range(len(preds)):\n",
        "            \n",
        "            sample = samples[i]\n",
        "            pred = preds[i]\n",
        "            \n",
        "            distance, neighbors_idxs = knn.kneighbors([sample], n_neighbors)\n",
        "            neighbors_samples = df_orig.iloc[neighbors_idxs[0]]\n",
        "            \n",
        "           \n",
        "            distance = [distance[0][1:]]\n",
        "            neighbors_idxs = [neighbors_idxs[0][1:]]\n",
        "            \n",
        "\n",
        "            # Distance to closest neighbors\n",
        "            if len(distance[0]) > 0 :\n",
        "                dst_mean = np.mean(distance[0])\n",
        "            else:\n",
        "                print('Error, no neighbor found')\n",
        "            mean_dists[i].append(dst_mean)\n",
        "            \n",
        "            neighbors_pts_target = np.array(neighbors_samples[target]).astype(int)\n",
        "            prop = list(neighbors_pts_target).count(pred)\n",
        "            prop_same_class[i].append(float(prop)/float(n_neighbors))\n",
        "                        \n",
        "    return mean_dists, prop_same_class\n",
        "\n",
        "def add_maj_neighbors_to(df_adv, df_orig, conf, knn, n_neighbors):\n",
        "    df_return = df_adv.copy()\n",
        "    \n",
        "    if 'mean_dists_at_org' in df_return.columns:\n",
        "        df_return = df_return.drop(columns='mean_dists_at_org')\n",
        "    if 'mean_dists_at_tgt' in df_return.columns:\n",
        "        df_return = df_return.drop(columns='mean_dists_at_tgt')\n",
        "    if 'prop_same_class_arg_org' in df_return.columns:\n",
        "        df_return = df_return.drop(columns='prop_same_class_arg_org')\n",
        "    if 'prop_same_class_arg_adv' in df_return.columns:\n",
        "        df_return = df_return.drop(columns='prop_same_class_arg_adv')\n",
        "        \n",
        "    mean_dists, prop_same_class = get_majority_neighbors(df_adv, df_orig, conf, knn, n_neighbors)\n",
        "    \n",
        "    df_return.insert(0, 'mean_dists_at_org', mean_dists[0])\n",
        "    df_return.insert(0, 'mean_dists_at_tgt', mean_dists[1])\n",
        "\n",
        "    df_return.insert(0, 'prop_same_class_arg_org', prop_same_class[0])\n",
        "    df_return.insert(0, 'prop_same_class_arg_adv', prop_same_class[1])\n",
        "    \n",
        "    return df_return\n",
        "\n",
        "def scale_data(conf, df_orig):\n",
        "    print('Before')\n",
        "    print(df.describe(include='all'))\n",
        "    print(weights)\n",
        "    for col, weight in zip(list(df.columns), weights):\n",
        "        df[col] = df[col].apply(lambda x: x * weight)\n",
        "        \n",
        "    bounds = [[bounds[i][x] * weight for x, weight in enumerate(weights)] for i in range(len(bounds))]\n",
        "    print(df.describe(include='all'))\n",
        "    return df, bounds\n",
        "\n",
        "def weighted_distance(x, y, w):\n",
        "    sum_ = 0\n",
        "    assert(len(x) == len(y) == len(w))\n",
        "    for i in range(len(x)):\n",
        "        sum_ += (w[i] * (y[i] - x[i])) ** 2\n",
        "    sum_ = np.sqrt(sum_)\n",
        "    return sum_\n",
        "\n",
        "def add_maj_neighbors(df_adv, df_orig, conf, n_neighbors):\n",
        "    # Otherwise we have issues because the KNN returns indexes in len(df) and not based on the real indexes on the samples\n",
        "    df_adv = df_adv.reset_index().drop(columns=['index'])\n",
        "    df_orig = df_orig.reset_index().drop(columns=['index'])\n",
        "    weights = conf['Weights']\n",
        "\n",
        "    assert(weights[0] > 0)\n",
        "\n",
        "    feature_names = conf['FeatureNames']\n",
        "    target = conf['Target']\n",
        "        \n",
        "        \n",
        "    knn = NearestNeighbors(n_neighbors=n_neighbors, metric='l2')\n",
        "    knn.fit(df_orig[feature_names], df_orig[target])\n",
        "    \n",
        "    knn_weighted = NearestNeighbors(n_neighbors=n_neighbors, metric=weighted_distance, metric_params={'w' : weights})\n",
        "    knn_weighted.fit(df_orig[feature_names], df_orig[target])\n",
        "\n",
        "    \n",
        "    df_adv_return = add_maj_neighbors_to(df_adv, df_orig, conf, knn, n_neighbors)\n",
        "    df_adv_weighted = add_maj_neighbors_to(df_adv, df_orig, conf, knn_weighted, n_neighbors)\n",
        "    \n",
        "    return df_adv_return, df_adv_weighted\n"
      ],
      "metadata": {
        "id": "Ok-iiYz6ucMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Misc\n",
        "import numpy as np\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "# from torch.autograd.gradcheck import zero_gradients\n",
        "\n",
        "# Clipping function\n",
        "def clip(current, low_bound, up_bound):\n",
        "    assert(len(current) == len(up_bound) and len(low_bound) == len(up_bound))\n",
        "    low_bound = torch.FloatTensor(low_bound)\n",
        "    up_bound = torch.FloatTensor(up_bound)\n",
        "    clipped = torch.max(torch.min(current, up_bound), low_bound)\n",
        "    return clipped\n",
        "\n",
        "\n",
        "def lowProFool(x, model, weights, bounds, maxiters, alpha, lambda_, cat_indices=[]):\n",
        "    \"\"\"\n",
        "    Generates an adversarial examples x' from an original sample x\n",
        "\n",
        "    :param x: tabular sample\n",
        "    :param model: neural network\n",
        "    :param weights: feature importance vector associated with the dataset at hand\n",
        "    :param bounds: bounds of the datasets with respect to each feature\n",
        "    :param maxiters: maximum number of iterations ran to generate the adversarial examples\n",
        "    :param alpha: scaling factor used to control the growth of the perturbation\n",
        "    :param lambda_: trade off factor between fooling the classifier and generating imperceptible adversarial example\n",
        "    :return: original label prediction, final label prediction, adversarial examples x', iteration at which the class changed\n",
        "    \"\"\"\n",
        "\n",
        "    r = Variable(torch.FloatTensor(1e-4 * np.ones(x.numpy().shape)), requires_grad=True) \n",
        "    v = torch.FloatTensor(np.array(weights))\n",
        "    \n",
        "    output = model.forward(x + r)\n",
        "    orig_pred = output.max(0, keepdim=True)[1].cpu().numpy()\n",
        "    target_pred = np.abs(1 - orig_pred)\n",
        "    \n",
        "    target = [0., 1.] if target_pred == 1 else [1., 0.]\n",
        "    target = Variable(torch.tensor(target, requires_grad=False)) \n",
        "    \n",
        "    lambda_ = torch.tensor([lambda_])\n",
        "    \n",
        "    bce = nn.BCELoss()\n",
        "    l1 = lambda v, r: torch.sum(torch.abs(v * r)) #L1 norm\n",
        "    l2 = lambda v, r: torch.sqrt(torch.sum(torch.mul(v * r,v * r))) #L2 norm\n",
        "    \n",
        "    best_norm_weighted = np.inf\n",
        "    best_pert_x = x\n",
        "    \n",
        "    loop_i, loop_change_class = 0, 0\n",
        "    while loop_i < maxiters:\n",
        "            \n",
        "        zero_gradients(r)\n",
        "\n",
        "        # Computing loss \n",
        "        loss_1 = bce(output, target)\n",
        "        loss_2 = l2(v, r)\n",
        "        loss = (loss_1 + lambda_ * loss_2)\n",
        "\n",
        "        # Get the gradient\n",
        "        loss.backward(retain_graph=True)\n",
        "        grad_r = r.grad.data.cpu().numpy().copy()\n",
        "        \n",
        "        # Guide perturbation to the negative of the gradient\n",
        "        ri = - grad_r\n",
        "    \n",
        "        # limit huge step\n",
        "        ri *= alpha\n",
        "\n",
        "        # Adds new perturbation to total perturbation\n",
        "        r = r.clone().detach().cpu().numpy() + ri\n",
        "        \n",
        "        # For later computation\n",
        "        r_norm_weighted = np.sum(np.abs(r * weights))\n",
        "        \n",
        "        # Ready to feed the model\n",
        "        r = Variable(torch.FloatTensor(r), requires_grad=True) \n",
        "        \n",
        "        # Compute adversarial example\n",
        "        xprime = x + r\n",
        "        \n",
        "        # Clip to stay in legitimate bounds\n",
        "        xprime = clip(xprime, bounds[0], bounds[1])\n",
        "        \n",
        "        # Classify adversarial example\n",
        "        output = model.forward(xprime)\n",
        "        output_pred = output.max(0, keepdim=True)[1].cpu().numpy()\n",
        "        \n",
        "        # Keep the best adverse at each iterations\n",
        "        if output_pred != orig_pred and r_norm_weighted < best_norm_weighted:\n",
        "            best_norm_weighted = r_norm_weighted\n",
        "            best_pert_x = xprime\n",
        "\n",
        "        if output_pred == orig_pred:\n",
        "            loop_change_class += 1\n",
        "            \n",
        "        loop_i += 1 \n",
        "        \n",
        "    # Clip at the end no matter what\n",
        "    best_pert_x = clip(best_pert_x, bounds[0], bounds[1])\n",
        "    output = model.forward(best_pert_x)\n",
        "    output_pred = output.max(0, keepdim=True)[1].cpu().numpy()\n",
        "\n",
        "    return orig_pred, output_pred, best_pert_x.clone().detach().cpu().numpy(), loop_change_class \n",
        "\n",
        "# Forked from https://github.com/LTS4/DeepFool\n",
        "def deepfool(x_old, net, maxiters, alpha, bounds, weights=[], overshoot=0.002):\n",
        "    \"\"\"\n",
        "    :param image: tabular sample\n",
        "    :param net: network \n",
        "    :param maxiters: maximum number of iterations ran to generate the adversarial examples\n",
        "    :param alpha: scaling factor used to control the growth of the perturbation\n",
        "    :param bounds: bounds of the datasets with respect to each feature\n",
        "    :param weights: feature importance vector associated with the dataset at hand\n",
        "    :param overshoot: used as a termination criterion to prevent vanishing updates (default = 0.02).\n",
        "    :return: minimal perturbation that fools the classifier, number of iterations that it required, new estimated_label and perturbed image\n",
        "    \"\"\"\n",
        "    \n",
        "    input_shape = x_old.numpy().shape\n",
        "    x = x_old.clone()\n",
        "    x = Variable(x, requires_grad=True)\n",
        "    \n",
        "    output = net.forward(x)\n",
        "    orig_pred = output.max(0, keepdim=True)[1] # get the index of the max log-probability\n",
        "\n",
        "    origin = Variable(torch.tensor([orig_pred], requires_grad=False))\n",
        "\n",
        "    I = []\n",
        "    if orig_pred == 0:\n",
        "        I = [0, 1]\n",
        "    else:\n",
        "        I = [1, 0]\n",
        "        \n",
        "    w = np.zeros(input_shape)\n",
        "    r_tot = np.zeros(input_shape)\n",
        "    \n",
        "    k_i = origin\n",
        " \n",
        "    loop_i = 0\n",
        "    while torch.eq(k_i, origin) and loop_i < maxiters:\n",
        "                \n",
        "        # Origin class\n",
        "        output[I[0]].backward(retain_graph=True)\n",
        "        grad_orig = x.grad.data.numpy().copy()\n",
        "        \n",
        "        # Target class\n",
        "        zero_gradients(x)\n",
        "        output[I[1]].backward(retain_graph=True)\n",
        "        cur_grad = x.grad.data.numpy().copy()\n",
        "            \n",
        "        # set new w and new f\n",
        "        w = cur_grad - grad_orig\n",
        "        f = (output[I[1]] - output[I[0]]).data.numpy()\n",
        "\n",
        "        pert = abs(f)/np.linalg.norm(w.flatten())\n",
        "    \n",
        "        # compute r_i and r_tot\n",
        "        # Added 1e-4 for numerical stability\n",
        "        r_i =  (pert+1e-4) * w / np.linalg.norm(w)   \n",
        "        \n",
        "        if len(weights) > 0:\n",
        "            r_i /= np.array(weights)\n",
        "\n",
        "        # limit huge step\n",
        "        r_i = alpha * r_i / np.linalg.norm(r_i) \n",
        "            \n",
        "        r_tot = np.float32(r_tot + r_i)\n",
        "        \n",
        "        \n",
        "        pert_x = x_old + (1 + overshoot) * torch.from_numpy(r_tot)\n",
        "\n",
        "        if len(bounds) > 0:\n",
        "            pert_x = clip(pert_x, bounds[0], bounds[1])\n",
        "                \n",
        "        x = Variable(pert_x, requires_grad=True)\n",
        " \n",
        "        output = net.forward(x)\n",
        "        \n",
        "        k_i = torch.tensor(np.argmax(output.data.cpu().numpy().flatten()))\n",
        "                    \n",
        "        loop_i += 1\n",
        "\n",
        "    r_tot = (1+overshoot)*r_tot    \n",
        "    pert_x = clip(pert_x, bounds[0], bounds[1])\n",
        "\n",
        "    return orig_pred, k_i, pert_x.cpu(), loop_i"
      ],
      "metadata": {
        "id": "MQSfPBNvufJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 0\n",
        "DATASET = 'Transactions'"
      ],
      "metadata": {
        "id": "Su_YlSgRuhuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_df(dataset):\n",
        "#     assert(dataset == 'credit-g')\n",
        "    \n",
        "#     dataset = fetch_openml(dataset)\n",
        "#     target = 'target'\n",
        "#     df = pd.DataFrame(data= np.c_[dataset['data'], dataset[target]], columns= dataset['feature_names'] + [target])  \n",
        "\n",
        "#     # Renaming target for training later\n",
        "#     df[target] = df[target].apply(lambda x: 0.0 if x == 'bad' or x == 0.0 else 1.0)\n",
        "\n",
        "#     # Subsetting features to keep only continuous, discrete and ordered categorical\n",
        "#     feature_names = ['checking_status', 'duration', 'credit_amount',\n",
        "#                  'savings_status','employment','installment_commitment',\n",
        "#                  'residence_since','age','existing_credits','num_dependents',\n",
        "#                  'own_telephone','foreign_worker']\n",
        "\n",
        "#     df = df[feature_names + [target]]\n",
        "\n",
        "#     # Casting to float for later purpose\n",
        "#     df = df.astype(float)\n",
        "#     return df, target, feature_names\n",
        "#def get_df(dataset):\n",
        " #   assert(dataset == 'credit-g')\n",
        "    \n",
        "  #  dataset = fetch_openml(dataset)\n",
        "   # target = 'target'\n",
        "    #df = pd.DataFrame(data= np.c_[dataset['data'], dataset[target]], columns= dataset['feature_names'] + [target])  \n",
        "\n",
        "    # Renaming target for training later\n",
        "    #df[target] = df[target].apply(lambda x: 0.0 if x == 'bad' or x == 0.0 else 1.0)\n",
        "\n",
        "    # Subsetting features to keep only continuous, discrete and ordered categorical\n",
        "    #feature_names = ['checking_status', 'duration', 'credit_amount',\n",
        "     #            'savings_status','employment','installment_commitment',\n",
        "      #           'residence_since','age','existing_credits','num_dependents',\n",
        "       #          'own_telephone','foreign_worker']\n",
        "\n",
        "    #df = df[feature_names + [target]]\n",
        "    #df['checking_status']=df['checking_status'].map({'<0':0, '0<=X<200':2, 'no checking':1, '>=200':3})\n",
        "    #df['savings_status']=df['savings_status'].map({'no known savings':0, '<100':1, '500<=X<1000':3, '>=1000':4, '100<=X<500':2})\n",
        "    #df['employment']=df['employment'].map({'>=7':4, '1<=X<4':2, '4<=X<7':3, 'unemployed':0, '<1':1})\n",
        "    #df['own_telephone']=df['own_telephone'].map({'yes':1, 'none':0})\n",
        "    #df['foreign_worker']=df['foreign_worker'].map({'yes':1, 'no':0})\n",
        "\n",
        "\n",
        "    # Casting to float for later purpose\n",
        "    #print(df.head())\n",
        "    #df = df['checking_status'].astype(float)\n",
        "    #df = df.astype(float)\n",
        "   # return df, target, feature_names\n",
        "\n",
        "\n",
        "def get_df2(ds):\n",
        "  df_new = pd.read_csv(\"/content/drive/MyDrive/transactions.csv\")\n",
        "  df_new.drop(['merchantCity','merchantState','merchantZip','echoBuffer','posOnPremises','recurringAuthInd', 'Unnamed: 0'],axis=1,inplace=True)\n",
        "  df_new.drop(['accountNumber','customerId','merchantName'],axis=1,inplace=True)\n",
        "  df_new['transactionDateTime'] = pd.to_datetime(df_new['transactionDateTime'])\n",
        "\n",
        "  df_new['transactionDateTime_month'] = df_new['transactionDateTime'].dt.month\n",
        "  df_new['transactionDateTime_hour'] = df_new['transactionDateTime'].dt.hour\n",
        "\n",
        "  df_new.drop('transactionDateTime',axis = 1,inplace = True)\n",
        "  df_new.drop('currentExpDate',axis = 1,inplace = True)\n",
        "  df_new.drop('accountOpenDate',axis = 1,inplace = True)\n",
        "  df_new.drop('dateOfLastAddressChange',axis = 1,inplace = True)\n",
        "\n",
        "\n",
        "  df_new['availableMoney'] = df_new['availableMoney']/df_new['creditLimit']\n",
        "  df_new['transactionAmount']=df_new['transactionAmount']/df_new['creditLimit']\n",
        "  df_new['currentBalance']=df_new['currentBalance']/df_new['creditLimit']\n",
        "  df_new['isCVVcorrect']=(df_new['cardCVV']==df_new['enteredCVV'])\n",
        "  df_new['isSameCountry']=(df_new['acqCountry']==df_new['merchantCountryCode'])\n",
        "\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  le = LabelEncoder()\n",
        "  var = ['posEntryMode','posConditionCode','merchantCategoryCode','transactionType','cardPresent','expirationDateKeyInMatch','isFraud']\n",
        "  for i in var:\n",
        "      df_new[i] = le.fit_transform(df_new[i])\n",
        "\n",
        "  df_new.drop(['enteredCVV','cardCVV','merchantCountryCode','acqCountry','cardLast4Digits','creditLimit'],axis=1,inplace=True)\n",
        "  X=df_new['isFraud'].copy()\n",
        "  df_new.drop(['isFraud'],inplace=True,axis=1)\n",
        "  df_new['isFraud'] = X\n",
        "\n",
        "  feature_names = ['availableMoney', 'transactionAmount', 'posEntryMode',\n",
        "       'posConditionCode', 'merchantCategoryCode', 'transactionType',\n",
        "       'currentBalance', 'cardPresent', 'expirationDateKeyInMatch',\n",
        "       'transactionDateTime_month','transactionDateTime_hour', 'isCVVcorrect', 'isSameCountry']\n",
        "  target = 'isFraud'\n",
        "  \n",
        "  return df_new, target, feature_names\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "ttzVq1EFuj9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(df, target, feature_names, bounds):\n",
        "    df_return = df.copy()\n",
        "    # Makes sure target does not need scaling\n",
        "    targets = np.unique(df[target].values)\n",
        "    assert(len(targets == 2) and 0. in targets and 1. in targets)\n",
        "    \n",
        "    scaler = MinMaxScaler()\n",
        "    X = df_return[feature_names]\n",
        "    scaler.fit(X)    \n",
        "    df_return[feature_names] = scaler.transform(X)\n",
        "    \n",
        "    lower_bounds = scaler.transform([bounds[0]])\n",
        "    upper_bounds = scaler.transform([bounds[1]])\n",
        "\n",
        "    return scaler, df_return, (lower_bounds[0], upper_bounds[0])\n",
        "\n",
        "def get_weights(df, target, show_heatmap=False):\n",
        "    def heatmap(cor):\n",
        "        plt.figure(figsize=(8,6))\n",
        "        sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
        "        plt.show()\n",
        "\n",
        "    cor = df.corr()\n",
        "    cor_target = abs(cor[target])\n",
        "\n",
        "    weights = cor_target[:-1] #removing target WARNING ASSUMES TARGET IS LAST\n",
        "    weights = weights / np.linalg.norm(weights)\n",
        "\n",
        "    if show_heatmap:\n",
        "        heatmap(cor)\n",
        "            \n",
        "    return weights.values\n",
        "\n",
        "def balance_df(df):\n",
        "    len_df_0, len_df_1 = len(df[df[target] == 0.]), len(df[df[target] == 1.])\n",
        "    df_0 = df[df[target] == 0.].sample(min(len_df_0, len_df_1), random_state=SEED)\n",
        "    df_1 = df[df[target] == 1.].sample(min(len_df_0, len_df_1), random_state=SEED)\n",
        "    df = pd.concat((df_0, df_1))\n",
        "    return df\n",
        "\n",
        "def get_bounds():\n",
        "    low_bounds = df_orig.min().values\n",
        "    up_bounds = df_orig.max().values\n",
        "    \n",
        "    #removing target WARNING ASSUMES TARGET IS LAST\n",
        "    low_bounds = low_bounds[:-1]\n",
        "    up_bounds = up_bounds[:-1]\n",
        "    \n",
        "    return [low_bounds, up_bounds]\n",
        "\n",
        "def split_train_test_valid(val_size=50,test_size=300):\n",
        "    # Train test splits\n",
        "    print(df.shape)\n",
        "    df_train, df_test = train_test_split(df, test_size=test_size, shuffle=True, random_state=SEED)\n",
        "    df_test, df_valid = train_test_split(df_test, test_size=val_size, shuffle=True, random_state=SEED)\n",
        "    \n",
        "    return df_train, df_test, df_valid"
      ],
      "metadata": {
        "id": "yuCv1pMUu0vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(conf, load=False):\n",
        "    assert(conf['Dataset'] == 'credit-g')\n",
        "    \n",
        "    class Linear(nn.Module):\n",
        "        def __init__(self, D_in, H, D_out):\n",
        "            super(Linear, self).__init__()\n",
        "            self.linear1 = torch.nn.Linear(D_in, H)\n",
        "            self.linear2 = torch.nn.Linear(H, H)\n",
        "            self.linear3 = torch.nn.Linear(H, D_out)\n",
        "            self.relu = torch.nn.ReLU()\n",
        "            self.softmax = torch.nn.Softmax(dim=0)\n",
        "\n",
        "        def forward(self, x):\n",
        "            h1 = self.relu(self.linear1(x))\n",
        "            h2 = self.relu(self.linear2(h1))\n",
        "            h3 = self.relu(self.linear2(h2))\n",
        "            h4 = self.relu(self.linear2(h3))\n",
        "            h5 = self.relu(self.linear2(h4))\n",
        "            h6 = self.relu(self.linear2(h5))\n",
        "            a3 = self.linear3(h6)\n",
        "            y = self.softmax(a3)\n",
        "            return y\n",
        "\n",
        "    def train(model, criterion, optimizer, X, y, N, n_classes):\n",
        "        model.train()\n",
        "\n",
        "        current_loss = 0\n",
        "        current_correct = 0\n",
        "\n",
        "\n",
        "        # Training in batches\n",
        "        for ind in range(0, X.size(0), N):\n",
        "            indices = range(ind, min(ind + N, X.size(0)) - 1) \n",
        "            inputs, labels = X[indices], y[indices]\n",
        "            inputs = Variable(inputs, requires_grad=True)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(inputs)\n",
        "            _, indices = torch.max(output, 1) # argmax of output [[0.61, 0.12]] -> [0]\n",
        "            # [[0, 1, 1, 0, 1, 0, 0]] -> [[1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0]]\n",
        "            preds = torch.tensor(keras.utils.to_categorical(indices, num_classes=n_classes))\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            current_loss += loss.item()\n",
        "            current_correct += (preds.int() == labels.int()).sum() /n_classes\n",
        "\n",
        "\n",
        "        current_loss = current_loss / X.size(0)\n",
        "        current_correct = current_correct.double() / X.size(0)    \n",
        "\n",
        "        return preds, current_loss, current_correct.item()\n",
        "    \n",
        "    df = conf['TrainData']\n",
        "    target = conf['Target']\n",
        "    feature_names = conf['FeatureNames']\n",
        "                        \n",
        "    n_classes = len(np.unique(df[target]))\n",
        "    X_train = torch.FloatTensor(df[feature_names].values)\n",
        "    y_train = keras.utils.to_categorical(df[target], n_classes)\n",
        "    y_train = torch.FloatTensor(y_train)\n",
        "\n",
        "    D_in = X_train.size(1)\n",
        "    D_out = y_train.size(1)\n",
        "\n",
        "    epochs = 400\n",
        "    batch_size = 100\n",
        "    H = 100\n",
        "    net = Linear(D_in, H, D_out)\n",
        "\n",
        "    lr = 1e-4    \n",
        "    criterion = torch.nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        preds, epoch_loss, epoch_acc = train(net, criterion, optimizer, X_train, y_train, batch_size, n_classes)     \n",
        "        if (epoch % 50 == 0):\n",
        "            print(\"> epoch {:.0f}\\tLoss {:.5f}\\tAcc {:.5f}\".format(epoch, epoch_loss, epoch_acc))\n",
        "\n",
        "    net.eval()\n",
        "    \n",
        "    return net"
      ],
      "metadata": {
        "id": "v8ENwzHPu3aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_adv(config, method):\n",
        "    df_test = config['TestData']\n",
        "    extra_cols = ['orig_pred', 'adv_pred', 'iters']    \n",
        "    model = config['Model']\n",
        "    weights = config['Weights']\n",
        "    bounds = config['Bounds']\n",
        "    maxiters = config['MaxIters']\n",
        "    alpha = config['Alpha']\n",
        "    lambda_ = config['Lambda']\n",
        "    \n",
        "    results = np.zeros((len(df_test), len(feature_names) + len(extra_cols)))    \n",
        "            \n",
        "    i = -1\n",
        "    for _, row in tqdm_notebook(df_test.iterrows(), total=df_test.shape[0], desc=\"{}\".format(method)):\n",
        "        i += 1\n",
        "        x_tensor = torch.FloatTensor(row[config['FeatureNames']])   \n",
        "        \n",
        "        if method == 'LowProFool':\n",
        "            orig_pred, adv_pred, x_adv, loop_i = lowProFool(x_tensor, model, weights, bounds,\n",
        "                                                             maxiters, alpha, lambda_)\n",
        "            if(i==1):\n",
        "              print(x_tensor, x_adv)\n",
        "\n",
        "        elif method == 'Deepfool':\n",
        "            orig_pred, adv_pred, x_adv, loop_i = deepfool(x_tensor, model, maxiters, alpha,\n",
        "                                                          bounds, weights=[])\n",
        "        else:\n",
        "            raise Exception(\"Invalid method\", method)\n",
        "        results[i] = np.concatenate((x_adv, [orig_pred, adv_pred, loop_i]), axis=0)\n",
        "        \n",
        "    return pd.DataFrame(results, index=df_test.index, columns = feature_names + extra_cols)"
      ],
      "metadata": {
        "id": "yL-MCMgBvCVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load initial dataset\n",
        "df_orig, target, feature_names = get_df2(DATASET)\n",
        "print(df_orig.columns)\n",
        "\n",
        "# Balance dataset classes\n",
        "df = balance_df(df_orig)\n",
        "\n",
        "# Compute the bounds for clipping\n",
        "bounds = get_bounds()\n",
        "\n",
        "# Normalize the data\n",
        "scaler, df, bounds = normalize(df, target, feature_names, bounds)\n",
        "\n",
        "# Compute the weihts modelizing the expert's knowledge\n",
        "weights = get_weights(df, target)\n",
        "\n",
        "# Split df into train/test/valid\n",
        "splits=int(df.shape[0]*0.2)\n",
        "df_train, df_test, df_valid = split_train_test_valid(int(splits/2),splits)\n",
        "\n",
        "# Build experimenation config\n",
        "config = {'Dataset'     : 'credit-g',\n",
        "         'MaxIters'     : 20000,\n",
        "         'Alpha'        : 0.001,\n",
        "         'Lambda'       : 8.5,\n",
        "         'TrainData'    : df_train,\n",
        "         'TestData'     : df_test,\n",
        "         'ValidData'    : df_valid,\n",
        "         'Scaler'       : scaler,\n",
        "         'FeatureNames' : feature_names,\n",
        "         'Target'       : target,\n",
        "         'Weights'      : weights,\n",
        "         'Bounds'       : bounds}\n",
        "\n",
        "# Train neural network\n",
        "model = get_model(config)\n",
        "config['Model'] = model\n",
        "\n",
        "# Compute accuracy on test set\n",
        "y_true = df_test[target]\n",
        "x_test = torch.FloatTensor(df_test[feature_names].values)\n",
        "y_pred = model(x_test)\n",
        "y_pred = np.argmax(y_pred.detach().numpy(), axis=1)\n",
        "print(\"Accuracy score on test data\", accuracy_score(y_true, y_pred))\n",
        "    \n",
        "# Sub sample\n",
        "config['TestData'] = config['TestData'].sample(n=150, random_state = SEED)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNOZhw4fvEyA",
        "outputId": "706e0dc6-f284-49aa-834a-b7e1bf0bb109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['availableMoney', 'transactionAmount', 'posEntryMode',\n",
            "       'posConditionCode', 'merchantCategoryCode', 'transactionType',\n",
            "       'currentBalance', 'cardPresent', 'expirationDateKeyInMatch',\n",
            "       'transactionDateTime_month', 'transactionDateTime_hour', 'isCVVcorrect',\n",
            "       'isSameCountry', 'isFraud'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24834, 14)\n",
            "> epoch 0\tLoss 0.02303\tAcc 0.57620\n",
            "> epoch 50\tLoss 0.02272\tAcc 0.64959\n",
            "> epoch 100\tLoss 0.02267\tAcc 0.65397\n",
            "> epoch 150\tLoss 0.02263\tAcc 0.66061\n",
            "> epoch 200\tLoss 0.02260\tAcc 0.66554\n",
            "> epoch 250\tLoss 0.02258\tAcc 0.67012\n",
            "> epoch 300\tLoss 0.02255\tAcc 0.67435\n",
            "> epoch 350\tLoss 0.02253\tAcc 0.67636\n",
            "Accuracy score on test data 0.6407571486105518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_adv2(config, method, cat_indices,max_vals,is_editable):\n",
        "    df_test = config['TestData']\n",
        "    extra_cols = ['orig_pred', 'adv_pred', 'iters']    \n",
        "    model = config['Model']\n",
        "    weights = config['Weights']\n",
        "    bounds = config['Bounds']\n",
        "    maxiters = config['MaxIters']\n",
        "    alpha = config['Alpha']\n",
        "    lambda_ = config['Lambda']\n",
        "    \n",
        "    results = np.zeros((len(df_test), len(feature_names) + len(extra_cols)))    \n",
        "            \n",
        "    i = -1\n",
        "    for _, row in tqdm_notebook(df_test.iterrows(), total=df_test.shape[0], desc=\"{}\".format(method)):\n",
        "        i += 1\n",
        "        x_tensor = torch.FloatTensor(row[config['FeatureNames']])   \n",
        "        \n",
        "        if method == 'LowProFool':\n",
        "            orig_pred, adv_pred, x_adv, loop_i = lowProFool2(x_tensor, model, weights, bounds,\n",
        "                                                             maxiters, alpha, lambda_,\n",
        "                                                             cat_indices,max_vals,is_editable)\n",
        "            if(i==1):\n",
        "              print(x_tensor, x_adv)\n",
        "\n",
        "        elif method == 'Deepfool':\n",
        "            orig_pred, adv_pred, x_adv, loop_i = deepfool(x_tensor, model, maxiters, alpha,\n",
        "                                                          bounds, weights=[])\n",
        "        else:\n",
        "            raise Exception(\"Invalid method\", method)\n",
        "        results[i] = np.concatenate((x_adv, [orig_pred, adv_pred, loop_i]), axis=0)\n",
        "        \n",
        "    return pd.DataFrame(results, index=df_test.index, columns = feature_names + extra_cols)\n",
        "\n",
        "def lowProFool2(x, model, weights, bounds, maxiters, alpha, lambda_, cat_indices,max_vals, is_editable):\n",
        "    \"\"\"\n",
        "    Generates an adversarial examples x' from an original sample x\n",
        "\n",
        "    :param x: tabular sample\n",
        "    :param model: neural network\n",
        "    :param weights: feature importance vector associated with the dataset at hand\n",
        "    :param bounds: bounds of the datasets with respect to each feature\n",
        "    :param maxiters: maximum number of iterations ran to generate the adversarial examples\n",
        "    :param alpha: scaling factor used to control the growth of the perturbation\n",
        "    :param lambda_: trade off factor between fooling the classifier and generating imperceptible adversarial example\n",
        "    :return: original label prediction, final label prediction, adversarial examples x', iteration at which the class changed\n",
        "    \"\"\"\n",
        "\n",
        "    r = Variable(torch.FloatTensor(1e-8 * np.ones(x.numpy().shape)), requires_grad=True) \n",
        "    v = torch.FloatTensor(np.array(weights))\n",
        "    num_idx = cat_indices\n",
        "    n_print_samples=5\n",
        "    n_print_counter=0\n",
        "    \n",
        "    output = model.forward(x + r)\n",
        "    orig_pred = output.max(0, keepdim=True)[1].cpu().numpy()\n",
        "    target_pred = np.abs(1 - orig_pred)\n",
        "    \n",
        "    target = [0., 1.] if target_pred == 1 else [1., 0.]\n",
        "    target = Variable(torch.tensor(target, requires_grad=False)) \n",
        "    \n",
        "    lambda_ = torch.tensor([lambda_])\n",
        "    \n",
        "    bce = nn.BCELoss()\n",
        "    l1 = lambda v, r: torch.sum(torch.abs(v * r)) #L1 norm\n",
        "    l2 = lambda v, r: torch.sqrt(torch.sum(torch.mul(v * r,v * r))) #L2 norm\n",
        "    \n",
        "    best_norm_weighted = np.inf\n",
        "    best_pert_x = x\n",
        "    \n",
        "    loop_i, loop_change_class = 0, 0\n",
        "    while loop_i < maxiters:\n",
        "            \n",
        "        zero_gradients(r)\n",
        "\n",
        "        # Computing loss \n",
        "        loss_1 = bce(output, target)\n",
        "        loss_2 = l2(v, r)\n",
        "        loss = (loss_1 + lambda_ * loss_2)\n",
        "\n",
        "        # Get the gradient\n",
        "        loss.backward(retain_graph=True)\n",
        "        grad_r = r.grad.data.cpu().numpy().copy()\n",
        "        \n",
        "        # Guide perturbation to the negative of the gradient\n",
        "        ri = - grad_r\n",
        "    \n",
        "        # limit huge step\n",
        "        ri *= alpha\n",
        "\n",
        "        # Adds new perturbation to total perturbation\n",
        "        r = r.clone().detach().cpu().numpy() + ri\n",
        "        \n",
        "        # For later computation\n",
        "        r_norm_weighted = np.sum(np.abs(r * weights))\n",
        "        \n",
        "        # Ready to feed the model\n",
        "        r = Variable(torch.FloatTensor(r), requires_grad=True) \n",
        "        \n",
        "        # Compute adversarial example\n",
        "        xprime = x + r\n",
        "        \n",
        "        # Clip to stay in legitimate bounds\n",
        "        xprime = clip(xprime, bounds[0], bounds[1])\n",
        "        # print(xprime)\n",
        "        # gfun = xprime.grad_fn\n",
        "        \n",
        "        counter = 0\n",
        "\n",
        "        # if n_print_counter<n_print_samples:\n",
        "        #     print(\"before mod\")\n",
        "        #     print(xprime,x)\n",
        "\n",
        "        for cat_idx, bnd, editable in zip( cat_indices, max_vals, is_editable):\n",
        "          # print(x.item())\n",
        "          # print(bnd)\n",
        "          if editable:\n",
        "            if cat_idx:\n",
        "              if xprime[counter].item()==0:\n",
        "                xprime[counter] = 0\n",
        "              else:\n",
        "\n",
        "                # if n_print_counter<n_print_samples:\n",
        "                #   print(counter, float(round(xprime[counter].item()*bnd)/bnd))\n",
        "\n",
        "                xprime[counter] = float(round(xprime[counter].item()*bnd)/bnd)\n",
        "          else:\n",
        "            xprime[counter]  = x[counter]\n",
        "          counter += 1\n",
        "          # print(counter)\n",
        "          # print(xprime)\n",
        "\n",
        "        # if n_print_counter<n_print_samples:\n",
        "        #   print(\"after mod\")\n",
        "        #   print(xprime)\n",
        "        #   n_print_counter+=1\n",
        "            \n",
        "\n",
        "        \n",
        "        # print(xprime)\n",
        "        \n",
        "        # Classify adversarial example\n",
        "        output = model.forward(xprime)\n",
        "        output_pred = output.max(0, keepdim=True)[1].cpu().numpy()\n",
        "        \n",
        "        # Keep the best adverse at each iterations\n",
        "        if output_pred != orig_pred and r_norm_weighted < best_norm_weighted:\n",
        "            best_norm_weighted = r_norm_weighted\n",
        "            best_pert_x = xprime\n",
        "\n",
        "        if output_pred == orig_pred:\n",
        "            loop_change_class += 1\n",
        "            \n",
        "        loop_i += 1 \n",
        "        \n",
        "    # Clip at the end no matter what\n",
        "    best_pert_x = clip(best_pert_x, bounds[0], bounds[1])\n",
        "    output = model.forward(best_pert_x)\n",
        "    output_pred = output.max(0, keepdim=True)[1].cpu().numpy()\n",
        "\n",
        "    return orig_pred, output_pred, best_pert_x.clone().detach().cpu().numpy(), loop_change_class \n"
      ],
      "metadata": {
        "id": "jAPtvF10wJ2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "config2 = deepcopy(config)\n",
        "config2['TestData'] = config2['TestData'].sample(n=150, random_state = SEED)\n",
        "max_vals = df_orig.max().astype(float).tolist()[:-1]\n",
        "is_editable = [False, True, False, False, True, False, False, False, False, True, True, True , True]\n",
        "\n",
        "lpf_test = gen_adv2(config2,\"LowProFool\",\n",
        "                    cat_indices=[False, False, True, True, True, True, False, True, True, True, True, True, True],\n",
        "                    max_vals=max_vals, is_editable=is_editable )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "26efdaf83be2471aab71a20b848a9e5b",
            "71bfb2d665a242a68515a98f46867ccc",
            "3b86a3ca198548c8bd7edd3ee630bfc9",
            "f4f301a017664476a87afa85a4032913",
            "0b18593212ca4633ba02c877c71d734b",
            "34da8fd33d73436c873de3e79f23c640",
            "bc6fbe2d6e3a442e8b880c2d924aeb44",
            "de5eccc509df4f268726bce77b500627",
            "d0709cf41db145279cb92b016be706fd",
            "d37ecaba209643ab868b9bcb4deca9c2",
            "fa789d9ae1904afaa11c5617620c6163"
          ]
        },
        "id": "gBTbmHHswZZY",
        "outputId": "a9727f40-c002-4362-af27-8e0e206fb552"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-82ebe7c618f2>:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for _, row in tqdm_notebook(df_test.iterrows(), total=df_test.shape[0], desc=\"{}\".format(method)):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26efdaf83be2471aab71a20b848a9e5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "LowProFool:   0%|          | 0/150 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.8284, 0.0199, 0.4000, 0.0000, 0.7222, 0.3333, 0.1716, 0.0000, 0.0000,\n",
            "        0.1818, 0.2174, 1.0000, 1.0000]) [0.82844895 0.01988091 0.4        0.         0.7222222  0.33333334\n",
            " 0.17155103 0.         0.         0.18181819 0.2173913  1.\n",
            " 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lpf_test['adv_pred'].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ciqlauvxxW8",
        "outputId": "b57491a9-9d5c-4b41-d007-5f1fc3b04fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    121\n",
              "0.0     29\n",
              "Name: adv_pred, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lpf_test['orig_pred'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p9eZbCe-nqm",
        "outputId": "505171af-c746-4d0f-c0bd-638cfd69c6a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    99\n",
              "0.0    51\n",
              "Name: orig_pred, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "vHPEyWoGx3pR",
        "outputId": "b3aafdc2-5708-4ee4-9d20-bc1dcd80ea78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        availableMoney  transactionAmount  posEntryMode  posConditionCode  \\\n",
              "493988        0.905367           0.392517           0.2          0.333333   \n",
              "612649        0.833570           0.217468           0.4          0.000000   \n",
              "376099        0.976335           0.011791           0.4          0.000000   \n",
              "688010        0.949952           0.003790           0.2          0.000000   \n",
              "155150        0.828919           0.000829           0.2          0.000000   \n",
              "\n",
              "        merchantCategoryCode  transactionType  currentBalance  cardPresent  \\\n",
              "493988              0.777778         0.333333        0.094633          0.0   \n",
              "612649              0.722222         0.333333        0.166430          0.0   \n",
              "376099              0.277778         0.666667        0.023665          1.0   \n",
              "688010              0.277778         0.333333        0.050048          1.0   \n",
              "155150              0.777778         0.333333        0.171081          0.0   \n",
              "\n",
              "        expirationDateKeyInMatch  transactionDateTime_month  \\\n",
              "493988                       0.0                   0.181818   \n",
              "612649                       0.0                   0.818182   \n",
              "376099                       0.0                   0.000000   \n",
              "688010                       0.0                   0.000000   \n",
              "155150                       0.0                   1.000000   \n",
              "\n",
              "        transactionDateTime_hour  isCVVcorrect  isSameCountry  isFraud  \n",
              "493988                  0.913043           1.0            1.0        1  \n",
              "612649                  0.260870           1.0            1.0        1  \n",
              "376099                  0.739130           1.0            1.0        1  \n",
              "688010                  0.695652           1.0            1.0        1  \n",
              "155150                  0.869565           1.0            1.0        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7911a648-758b-4498-9c7f-219a8fe081bc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>availableMoney</th>\n",
              "      <th>transactionAmount</th>\n",
              "      <th>posEntryMode</th>\n",
              "      <th>posConditionCode</th>\n",
              "      <th>merchantCategoryCode</th>\n",
              "      <th>transactionType</th>\n",
              "      <th>currentBalance</th>\n",
              "      <th>cardPresent</th>\n",
              "      <th>expirationDateKeyInMatch</th>\n",
              "      <th>transactionDateTime_month</th>\n",
              "      <th>transactionDateTime_hour</th>\n",
              "      <th>isCVVcorrect</th>\n",
              "      <th>isSameCountry</th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>493988</th>\n",
              "      <td>0.905367</td>\n",
              "      <td>0.392517</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.094633</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>612649</th>\n",
              "      <td>0.833570</td>\n",
              "      <td>0.217468</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166430</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.260870</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376099</th>\n",
              "      <td>0.976335</td>\n",
              "      <td>0.011791</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.023665</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.739130</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>688010</th>\n",
              "      <td>0.949952</td>\n",
              "      <td>0.003790</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.050048</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.695652</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155150</th>\n",
              "      <td>0.828919</td>\n",
              "      <td>0.000829</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.171081</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7911a648-758b-4498-9c7f-219a8fe081bc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7911a648-758b-4498-9c7f-219a8fe081bc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7911a648-758b-4498-9c7f-219a8fe081bc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['posEntryMode'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxwkRhgl6t0q",
        "outputId": "b7f2a853-1eb5-4bad-97af-d3e06d4931f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4    907\n",
              "0.2    771\n",
              "0.0    651\n",
              "0.8     65\n",
              "0.6     60\n",
              "1.0     29\n",
              "Name: posEntryMode, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lpf_test['posEntryMode'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S64EbkpG5fUQ",
        "outputId": "bdf10117-b761-4be0-c2b5-8a57cd69ceb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4    58\n",
              "0.2    41\n",
              "0.0    40\n",
              "0.6     6\n",
              "0.8     3\n",
              "1.0     2\n",
              "Name: posEntryMode, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lpf_test['posConditionCode'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-f_zAc-A0_Y",
        "outputId": "d332fcfd-13b8-4192-fda6-e7fd034d0ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.66666669, 0.33333334])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['posConditionCode'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-vDJg6kAu1I",
        "outputId": "89dae364-1560-4562-9c67-a93adb8c9b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.000000    1987\n",
              "0.333333     454\n",
              "0.666667      39\n",
              "1.000000       3\n",
              "Name: posConditionCode, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5AaG9BXBUiZ",
        "outputId": "c7fffa70-ae1f-4dda-ee19-b998a58f9d3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2483, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lpf_test['cardPresent'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-UghLkAB2hp",
        "outputId": "8962ef8e-250e-4011-b447-13215370ae3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['cardPresent'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CK8b6jjB_px",
        "outputId": "acc456d9-10ef-4833-c368-ab4817678509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lpf_test['isCVVcorrect'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9knQIlqTCIEE",
        "outputId": "87a696f6-a50d-4724-cff5-ccdc10b43abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['isCVVcorrect'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glp9W8zoCP3i",
        "outputId": "9c50baec-a340-4d58-dc87-94a8304de669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['isSameCountry'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBrbwJiICTcc",
        "outputId": "0e7ea4f7-d6a6-4621-fdec-94281ff74460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lpf_test['isSameCountry'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYmn_ah_CXmT",
        "outputId": "25370843-bb85-4789-c705-0e017f73ff39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}